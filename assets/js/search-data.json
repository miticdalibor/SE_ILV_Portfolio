{
  
    
        "post0": {
            "title": "01 Requirements",
            "content": "Funktionale Requirements . Unter der Bedingung, dass der User alle Features des importierten Datensatzes über das UI kategorisiert hat und die Funktion &quot;Free Lunch&quot; gestartet hat, muss die Pipeline des Systems in der Lage sein, den Datensatz eigenständiig für das Modelltraining vorzubereiten. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, Spalten mit nur fehlenden Daten abzufragen und aus dem Datensatz zu löschen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, fehlende numerische Daten in einer Zeitserie durch einen Extremwert zu ersetzen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, fehlende kategorische Daten aus dem Datensatz zu entfernen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, entsprechend die numerischen Features zu skalieren zwischen 0 und 1 (MinMaxScaler). . | Wenn der vom User importierte Datensatz durch das Starten der &quot;Frunch Infinity 2.0&quot; Funktion vorverarbeitet wurde, sollen unterschiedliche konventionelle Modelle trainiert werden. . | . User Interface . Wenn der User die App startet, soll das System fähig sein, individuelle Datensätze zu importieren. . | Wenn der User einen beliebigen Datensatz über das UI importiert, muss die App &quot;Frunch Infinity 2.0&quot; die Möglichkeit bieten, die Feature-Bezeichnungen dem User anzuzeigen und der User soll entsprechend jedes Feature als numerisch oder kategorisch sowie die Zielvariable auswählen. . | Unter der Bedingung, dass der User alle Features des importierten Datensatzes über das UI kategorisiert hat und die Zielvariable kategorisch ist, soll die Applikation die Anzahl der unterschiedlichen Klassen in der Zielvariable sowie die Klassenbalance anzeigen. . | Wenn der User die Funktion &quot;Frunch Infinity 2.0&quot; gestartet hat, soll die Applikation fähig sein, dem User eine Auswahl von Metriken zum Vergleich der trainierten Modelle anbieten. . | Wenn das Training mit unterschiedlichen Modellen abgeschlossen ist, sollen die Ergebnisse mit den ausgewählten Metriken in einer Tabelle dem User über das UI angezeigt werden. . | Wenn der User die Applikation startet, soll das System die Möglichkeit anbieten, den Log einzusehen und zu exportieren von der APP. . | . UML Diagramm der zentralen Requirements . . Änderungen von Requirements . Da die gesamte Implementierung nach dem Agile-Prinzip über das Kanban-Board von Gitlab erfolgt (siehe Beispiel in der nachfolgenden Abbildung), können entsprechende Änderungen von Requirements über dieses Board eingefangen und koordiniert werden. . . Dabei kann das neue Requirement oder geänderte Requirement als neues Issue im Produkt-Backlog aufgenommen werden. Um dieses Issue von den anderen zu unterscheiden, soll in der Beschreibung des Issues eine Change-Request ID hinzugefügt werden (z.B. CR-001). Im nächsten Sprint soll entsprechend der Impact bewertet werden. Ist der Impact zu groß, muss entsprechend mit dem Kunden Rücksprache gehalten werden. Wenn der Implementierungsplan aus dem geänderten Requirement aufegesetzt ist, dann werden daraus ableitend entsprechend neues Issues für das Projektteam angelegt. Jedes der Issues welche aus dem CR-001 erstellt wurden, werden entsprechend mit einem Flag markiert, um nach der fertigen Umsetzung erkennen zu können, wann der CR-001 abgeschlossen wurde und die Dokumentation der Requirements wird abschließend angepasst. . Mögliche Veränderungen von Requirements wären beispielsweise: . Neue Edge-Cases in den zu importierenden Datensätzen, die die Pipeline verarbeiten muss. . | Model-Explanation: Der User will das Training der Modelle überwachen und dies muss entsprechend über das UI ausgegeben oder in einem Log-File gespeichert werden. . | Das beste Modell soll gespeichert werden, damit der User dies auf einem ähnlichen Datensatz anwendet. . | .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/04/requirements.html",
            "relUrl": "/markdown/2022/07/04/requirements.html",
            "date": " • Jul 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "02 Data Science Methodik-Konzept",
            "content": "CRISP-DM . Als Data-Science Konzept kann hier CRISP-DM angewendet werden (siehe Abbildung). Da Frunch-Infinity jedoch eine Auto-ML Lösung darstellt, ist der Business Understanding Teil nicht prioritär. Der Data Understanding Teil bleibt bei Frunch Infinity 2.0 auf der User Seite, da dieser über das UI entsprechend die Features sowie die Zielvariable im Datensatz definieren muss. Außerdem soll der User durch die Anwendung von Frunch-Infinity 2.0 einen Einblick in die Qualität der verwendeten Daten kriegen und somit das Verständnis der Daten gewährleisten. Am meisten Entwicklungsaufwand für Frunch-Infinity 2.0 wird der Teil &quot;Data Preprocessing&quot; sein, da die Pipeline unterschiedlichste Datensätze und Edge-Cases verarbeiten muss. . . Konzept Datenfluss . Die nachfolgende Grafik zeigt eine Übersicht des Datenflusses. Die gelb markierten Blöcke befinden sich im Frontend und die blau markierten Blöcke im Backend. . . Der User soll dabei die Daten aus dem lokalen Speicher in einem .csv Format über die Web-App hochladen. Über die Web-App soll der User dann die Feature-Bezeichnungen definieren und entsprechend als numerisch oder kategorisch markieren sowie die Zielvariable. Sobald die Funktion gestartet wird, werden die Daten entsprechend über eine Pipeline preprocessed und im Datawarehouse abgelegt. Von dort aus werden die preprocessed Daten geladen, die Modelle trainiert und die Ergebnisse entsprechend gespeichert und ausgegeben. .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/03/DataScienceMethodik-Konzept.html",
            "relUrl": "/markdown/2022/07/03/DataScienceMethodik-Konzept.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "03 Web Technologien",
            "content": "Web Technologien . Die Web-Architektur für Frunch Infinity 2.0 stellt eine Client-Server dar. Dabei soll der User entsprechend über den Client die Applikation im Browser öffnen und dort dessen Datensatz importieren und die Ergebnisse entsprechend lesen können. Das Trainieren der Modelle läuft im Backend auf der Server-Seite. Deshalb ist es notwendig hier den Server entsprechend für hohe Rechenleistungen auszulegen, damti auch große Datensätze verarbeitet werden können. Das Backend wird in Python implementiert und die Daten werden entsprechend in einem Data-Warehouse abgelegt. Das Frontend wird in React umgesetzt, um flexibel in der Implementierung der Web-APP zu sein und auf spezifische User-Anforderungen reagieren zu können. Das Frontend benötigt die Daten vom Backend im .json Format, damit es diese in der Web-APP verarbeiten kann. Hierzu wird FastAPI implementiert, da dieses sehr einfach implementiert werden kann und entsprechende Flexibilität im Datenaustausch bietet. Außerdem ist FastAPI sehr performant. Eine APP ist für Frunch Infinity 2.0 nicht erforderlich, da die Datensätze statt auf Handys auf Rechnern abgespeichert werden. Schnittstellen zu anderen Webseiten und Web-Scraper sind hier auch nicht erforderlich, da das Ziel von Frunch Infinity 2.0 ist, einen schnellen Einblick in die Datenqualität und -verwendbarkeit ohne hohen Programmieraufwand zu geben. Die Daten werden hauptsächlich im Data-Warehouse, welches im Backend liegt gespeichert. Da Frunch Infinity 2.0 eine Enterprise Solution ist, soll das System im firmeneigenen Intranet aufgesetzt werden, damit die Kunden die Daten nicht nach außen über Cloud-Solutions vergeben müssen. Deshalb muss auch die Verwaltung und das Management der Server-Hardware vom Kunden erfolgen oder entsprechende Service Level Agreements mit Frunch Infinity aufgesetzt werden, die die Wartung des Systems übernehmen. Für das Datawarehouse können keine besonderen Anforderungen außer der Konsistenz der Daten abgeleitet werden. Die Zeit für das Schreiben und Lesen der Daten in und vom Datawarehouse ist im Vergleich zur Dauer des Modelltrainings marginal. Es ist aber wichtig, dass die Daten konsistent sind, damit die Integrität der Daten gegeben ist über den gesamten Lebenszyklus. Deshalb wird als Datawarehouse eine SQL-Datenbank definiert. Der Zugriff auf das Datawarehouse kann über SQLAlchemy erfolgen. Für die Verwaltung der Daten im Datawarehouse wird DVC (Data Version Control) verwendet. .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/02/WebTechnologien.html",
            "relUrl": "/markdown/2022/07/02/WebTechnologien.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "04 Architektur",
            "content": "Web Technologien . Die Web-Architektur für Frunch Infinity 2.0 stellt eine Client-Server dar. Dabei soll der User entsprechend über den Client die Applikation im Browser öffnen und dort dessen Datensatz importieren und die Ergebnisse entsprechend lesen können. Das Trainieren der Modelle läuft im Backend auf der Server-Seite. Deshalb ist es notwendig hier den Server entsprechend für hohe Rechenleistungen auszulegen, damti auch große Datensätze verarbeitet werden können. Das Backend wird in Python implementiert und die Daten werden entsprechend in einem Data-Warehouse abgelegt. Das Frontend wird in React umgesetzt, um flexibel in der Implementierung der Web-APP zu sein und auf spezifische User-Anforderungen reagieren zu können. Das Frontend benötigt die Daten vom Backend im .json Format, damit es diese in der Web-APP verarbeiten kann. Hierzu wird FastAPI implementiert, da dieses sehr einfach implementiert werden kann und entsprechende Flexibilität im Datenaustausch bietet. Außerdem ist FastAPI sehr performant. Eine APP ist für Frunch Infinity 2.0 nicht erforderlich, da die Datensätze statt auf Handys auf Rechnern abgespeichert werden. Schnittstellen zu anderen Webseiten und Web-Scraper sind hier auch nicht erforderlich, da das Ziel von Frunch Infinity 2.0 ist, einen schnellen Einblick in die Datenqualität und -verwendbarkeit ohne hohen Programmieraufwand zu geben. Die Daten werden hauptsächlich im Data-Warehouse, welches im Backend liegt gespeichert. Da Frunch Infinity 2.0 eine Enterprise Solution ist, soll das System im firmeneigenen Intranet aufgesetzt werden, damit die Kunden die Daten nicht nach außen über Cloud-Solutions vergeben müssen. Deshalb muss auch die Verwaltung und das Management der Server-Hardware vom Kunden erfolgen oder entsprechende Service Level Agreements mit Frunch Infinity aufgesetzt werden, die die Wartung des Systems übernehmen. Basierend auf den Anforderungen und dem geplanten Einsatz von Frunch Infinity 2.0 können daraus folgende Anforderungen für das Datawarehouse abgeleitet werden: - .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/01/Architektur.html",
            "relUrl": "/markdown/2022/07/01/Architektur.html",
            "date": " • Jul 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Name: Ing. Dalibor Mitic, BSc. . Kurs: Softwareentiwcklung 2 . Jahrgang: DSIA 2021/22 . Studiengang: Data Science &amp; Intelligent Analytics . Institut: Fachhochschule Kufstein . This website is powered by fastpages. .",
          "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}