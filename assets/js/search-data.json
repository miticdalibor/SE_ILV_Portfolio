{
  
    
        "post0": {
            "title": "01 Requirements",
            "content": "Funktionale Requirements . Unter der Bedingung, dass der User alle Features des importierten Datensatzes über das UI kategorisiert hat und die Funktion &quot;Free Lunch&quot; gestartet hat, muss die Pipeline des Systems in der Lage sein, den Datensatz eigenständiig für das Modelltraining vorzubereiten. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, Spalten mit nur fehlenden Daten abzufragen und aus dem Datensatz zu löschen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, fehlende numerische Daten in einer Zeitserie durch einen Extremwert zu ersetzen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, fehlende kategorische Daten aus dem Datensatz zu entfernen. . | Wenn der importierte Datensatz der Pipeline übergeben wurde, soll die Pipeline fähig sein, entsprechend die numerischen Features zu skalieren zwischen 0 und 1 (MinMaxScaler). . | Wenn der vom User importierte Datensatz durch das Starten der &quot;Frunch Infinity 2.0&quot; Funktion vorverarbeitet wurde, sollen unterschiedliche konventionelle Modelle trainiert werden. . | . User Interface . Wenn der User die App startet, soll das System fähig sein, individuelle Datensätze zu importieren. . | Wenn der User einen beliebigen Datensatz über das UI importiert, muss die App &quot;Frunch Infinity 2.0&quot; die Möglichkeit bieten, die Feature-Bezeichnungen dem User anzuzeigen und der User soll entsprechend jedes Feature als numerisch oder kategorisch sowie die Zielvariable auswählen. . | Unter der Bedingung, dass der User alle Features des importierten Datensatzes über das UI kategorisiert hat und die Zielvariable kategorisch ist, soll die Applikation die Anzahl der unterschiedlichen Klassen in der Zielvariable sowie die Klassenbalance anzeigen. . | Wenn der User die Funktion &quot;Frunch Infinity 2.0&quot; gestartet hat, soll die Applikation fähig sein, dem User eine Auswahl von Metriken zum Vergleich der trainierten Modelle anbieten. . | Wenn das Training mit unterschiedlichen Modellen abgeschlossen ist, sollen die Ergebnisse mit den ausgewählten Metriken in einer Tabelle dem User über das UI angezeigt werden. . | Wenn der User die Applikation startet, soll das System die Möglichkeit anbieten, den Log einzusehen und zu exportieren von der APP. . | . UML Diagramm der zentralen Requirements . . Änderungen von Requirements . Da die gesamte Implementierung nach dem Agile-Prinzip über das Kanban-Board von Gitlab erfolgt (siehe Beispiel in der nachfolgenden Abbildung), können entsprechende Änderungen von Requirements über dieses Board eingefangen und koordiniert werden. . . Dabei kann das neue Requirement oder geänderte Requirement als neues Issue im Produkt-Backlog aufgenommen werden. Um dieses Issue von den anderen zu unterscheiden, soll in der Beschreibung des Issues eine Change-Request ID hinzugefügt werden (z.B. CR-001). Im nächsten Sprint soll entsprechend der Impact bewertet werden. Ist der Impact zu groß, muss entsprechend mit dem Kunden Rücksprache gehalten werden. Wenn der Implementierungsplan aus dem geänderten Requirement aufegesetzt ist, dann werden daraus ableitend entsprechend neues Issues für das Projektteam angelegt. Jedes der Issues welche aus dem CR-001 erstellt wurden, werden entsprechend mit einem Flag markiert, um nach der fertigen Umsetzung erkennen zu können, wann der CR-001 abgeschlossen wurde und die Dokumentation der Requirements wird abschließend angepasst. . Mögliche Veränderungen von Requirements wären beispielsweise: . Neue Edge-Cases in den zu importierenden Datensätzen, die die Pipeline verarbeiten muss. . | Model-Explanation: Der User will das Training der Modelle überwachen und dies muss entsprechend über das UI ausgegeben oder in einem Log-File gespeichert werden. . | Das beste Modell soll gespeichert werden, damit der User dies auf einem ähnlichen Datensatz anwendet. . | . Diskussion .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/04/requirements.html",
            "relUrl": "/markdown/2022/07/04/requirements.html",
            "date": " • Jul 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "02 Data Science Methodik-Konzept",
            "content": "CRISP-DM . Als Data-Science Konzept kann hier CRISP-DM angewendet werden (siehe Abbildung). Da Frunch-Infinity jedoch eine Auto-ML Lösung darstellt, ist der Business Understanding Teil nicht prioritär. Der Data Understanding Teil bleibt bei Frunch Infinity 2.0 auf der User Seite, da dieser über das UI entsprechend die Features sowie die Zielvariable im Datensatz definieren muss. Außerdem soll der User durch die Anwendung von Frunch-Infinity 2.0 einen Einblick in die Qualität der verwendeten Daten kriegen und somit das Verständnis der Daten gewährleisten. Am meisten Entwicklungsaufwand für Frunch-Infinity 2.0 wird der Teil &quot;Data Preprocessing&quot; sein, da die Pipeline unterschiedlichste Datensätze und Edge-Cases verarbeiten muss. . . Diskussion .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/03/DataScienceMethodik-Konzept.html",
            "relUrl": "/markdown/2022/07/03/DataScienceMethodik-Konzept.html",
            "date": " • Jul 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "03 Architektur",
            "content": "Systemarchitektur . Die Systemarchitektur für Frunch Infinity 2.0 stellt eine Client-Server-Architektur dar. Dabei soll der User entsprechend über den Client die Applikation im Browser öffnen und dort dessen Datensatz importieren und die Ergebnisse entsprechend lesen können. Das Trainieren der Modelle läuft im Backend auf der Server-Seite. Deshalb ist es notwendig hier den Server entsprechend für hohe Rechenleistungen auszulegen, damit auch große Datensätze verarbeitet werden können. Das Backend wird in Python implementiert und die Daten werden entsprechend in einem Data-Warehouse abgelegt. Das Frontend wird in React umgesetzt, um flexibel in der Implementierung der Web-App zu sein und auf spezifische User-Anforderungen reagieren zu können. Das Frontend benötigt die Daten vom Backend im .json Format, damit es diese in der Web-App verarbeiten kann. Hierzu wird FastAPI implementiert, da dieses sehr einfach implementiert werden kann und entsprechende Flexibilität im Datenaustausch bietet. Außerdem ist FastAPI sehr performant und generiert automatisch eine Dokumentation. Eine mobile App ist für Frunch Infinity 2.0 nicht erforderlich, da die Datensätze auf Rechnern abgespeichert werden. Schnittstellen zu anderen Webseiten und Web-Scraper sind hier auch nicht erforderlich. Die Daten werden hauptsächlich im Data-Warehouse, welches im Backend liegt, gespeichert. Da Frunch Infinity 2.0 eine Enterprise Solution ist, soll das System im firmeneigenen Intranet aufgesetzt werden, damit die Kunden die Daten nicht nach außen über Cloud-Solutions vergeben müssen. Außerdem ist für Frunch-Infinity kein vortrainiertes Modell oder Transfer-Learning von geteilten Daten erforderlich. Deshalb muss auch die Verwaltung und das Management der Server-Hardware vom Kunden erfolgen. Für das Datawarehouse können keine besonderen Anforderungen außer der Konsistenz der Daten abgeleitet werden. Die Zeit für das Schreiben und Lesen der Daten im Datawarehouse ist im Vergleich zur Trainingszeit der Modelle marginal. Es ist aber wichtig, dass die Daten konsistent sind, damit die Integrität der Daten über den gesamten Lebenszyklus gegeben ist. Deshalb wird als Datawarehouse eine SQL-Datenbank definiert. Der Zugriff auf das Datawarehouse kann über SQLAlchemy erfolgen und für die Verwaltung der Datensätze wird DVC (Data Version Control) verwendet. Die nachfolgende Grafik zeigt eine Übersicht des Datenflusses. Die gelb markierten Blöcke befinden sich im Frontend und die blau markierten Blöcke im Backend. . . Der User soll dabei die Daten aus dem lokalen Speicher in einem .csv Format über die Web-App (Client) hochladen. Über die Web-App soll der User dann die Feature-Bezeichnungen definieren und entsprechend als numerisch oder kategorisch markieren, sowie die Zielvariable auswählen. Sobald die Funktion gestartet wird, werden die Daten entsprechend über eine Pipeline preprocessed und im Datawarehouse abgelegt (Server). Von dort aus werden die preprocessed Daten geladen, die Modelle trainiert und die Ergebnisse entsprechend gespeichert und ausgegeben. . CI/CD Pipeline . Die CI/CD Pipeline für Frunch Infinity 2.0 enthält folgende Stages: . Test: | Build: | Deploy: | . Diskussion .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/02/Architektur.html",
            "relUrl": "/markdown/2022/07/02/Architektur.html",
            "date": " • Jul 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "04 Design",
            "content": "Design Pattern . Für Frunch Infinity 2.0 werden das Factory Design Pattern (Creational patterns) sowie Decorators (Structural patterns) eingesetzt. Beispiel: Damit die Preprocessing-Pipeline unterschiedliche Datensätze verarbeiten kann, muss diese durch unterschiedliche Klassen implementiert werden, welche entsprechende Basis-Methoden enthalten und durch Edge-Cases erweitert werden können. Dabei werden entsprechend einige Klassen mit einem Decorator ausgestattet. Durch entsprechendes Refactoring soll der Code entsprechend strukturiert und sauber gehalten werden. Mit den gewählten Design Patterns und Refactoring soll das System skalierbar sein. . Tools . Neben den Tools bzw. Libraries, welche zur Entwicklung des Systems angewendet werden (siehe Abbildung &quot;Systemarchitektur und Tools&quot;) werden folgende Tools unterstützen für die Umsetzung angewendet: . Poetry: Wird zum Package Management und Dependencies angewendet. . - Vorteil: Poetry übernimmt die automatische Überprüfung der Dependencies und spart dem Entwickler viel Zeit. . | Hydra: Management von Pfaden sowie die zentrale Konfigurationen. . - Vorteil: Dadurch ist das System flexibler und Änderungen von Anforderungen können einfacher umgesetzt werden. . | Cookiecutter: Vorgabe der Repository-Struktur und einiger Basic Tools. . - Vorteil: Gute Basis für die Anfangsphase der Entwicklung und gibt einen Weg bzw. eine Struktur vor. Enthält einige Tools (z.B. black, flake8, isort, nbstripout etc.), welche den Code durch Überprüfungen während des Commits clean halten. . | Docker: Erstellt eine virtuelle Umgebung für eine einfachere Entwicklung und Build. . | Logging: Entsprechende Schritte im System werden geloggt, um ein leichteres Debugging zu gewährleisten. . - Beispiele: Ausgabe vom Modelltraining, Schreiben/Lese in die DB etc. . | Unit tests: Test Driven Development (TDD) . - Beispiel: Die Klassen der Pipeline müssen auf assert error getestet. . | Pdoc: Automatische Dokumentation von Klassen (Docstrings nur in English). . | .",
            "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/markdown/2022/07/01/Design.html",
            "relUrl": "/markdown/2022/07/01/Design.html",
            "date": " • Jul 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Name: Ing. Dalibor Mitic, BSc. . Kurs: Softwareentiwcklung 2 . Jahrgang: DSIA 2021/22 . Studiengang: Data Science &amp; Intelligent Analytics . Institut: Fachhochschule Kufstein . This website is powered by fastpages. .",
          "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://miticdalibor.github.io/SE_ILV_Portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}